<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>D.自动微分 - YShandon</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "D.\u81ea\u52a8\u5fae\u5206";
    var mkdocs_page_input_path = "D.\u81ea\u52a8\u5fae\u5206.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> YShandon</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../0.%E5%89%8D%E8%A8%80/">Home</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../0.%E5%89%8D%E8%A8%80/">0.前言</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../1.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/">1.机器学习概览</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../2.%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/">2.一个完整的机器学习项目</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../3.%E5%88%86%E7%B1%BB/">3.分类</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../4.%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">4.训练模型</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../5.%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">5.支持向量机</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../6.%E5%86%B3%E7%AD%96%E6%A0%91/">6.决策树</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../7.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/">7.集成学习和随机森林</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../8.%E9%99%8D%E7%BB%B4/">8.降维</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../9.%E5%90%AF%E5%8A%A8%E5%B9%B6%E8%BF%90%E8%A1%8C_TensorFlow/">9.启动并运行_TensorFlow</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../10.%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/">10.人工神经网络介绍</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../11.%E8%AE%AD%E7%BB%83%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">11.训练深层神经网络</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../12.%E8%AE%BE%E5%A4%87%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F_TensorFlow/">12.设备和服务器上的分布式_TensorFlow</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../13.%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">13.卷积神经网络</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../14.%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">14.循环神经网络</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../15.%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/">15.自编码器</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../16.%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">16.强化学习</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../C.SVM_%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98/">C.SVM_对偶问题</a>
                    </li>
                </ul>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">D.自动微分</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#_1">手动微分法</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#_2">符号微分</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#_3">数值微分</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#_4">前向自动微分</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#_5">反向自动微分</a>
    </li>
    </ul>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">YShandon</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>D.自动微分</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="d">附录 D、自动微分</h1>
<p>这个附录解释了 TensorFlow 的自动微分功能是如何工作的，以及它与其他解决方案的对比。</p>
<p>假定你定义了函数 <img alt="f(x, y) = x^2y + y + 2" src="../images/tex-162751afe7e0aa904426973dbac3654e.gif" />，需要得到它的偏导数 <img alt="\frac{\partial f}{\partial x}" src="../images/tex-f6e0346d1d3410b0fbe32b41b85999aa.gif" /> 和 <img alt="\frac{\partial f}{\partial y}" src="../images/tex-408378e8bc55170258126d10000c53d9.gif" />，以用于梯度下降或者其他优化算法。你的可选方案有手动微分法，符号微分法，数值微分法，前向自动微分，和反向自动微分。TensorFlow 实现的反向自动微分法。我们来看看每种方案。</p>
<h2 id="_1">手动微分法</h2>
<p>第一个方法是拿起一直笔和一张纸，使用你的代数知识去手动的求偏导数。对于已定义的函数，求它的偏导并不太困难。你需要使用如下 5 条规则：</p>
<ul>
<li>常数的导数为 0。</li>
<li><img alt="\lambda x" src="../images/tex-d08b62e799e1ff8f24464dc26a2daebe.gif" /> 的导数为 <img alt="\lambda" src="../images/tex-c6a6eb61fd9c6c913da73b3642ca147d.gif" />，<img alt="\lambda" src="../images/tex-c6a6eb61fd9c6c913da73b3642ca147d.gif" /> 为常数。</li>
<li><img alt="x^{\lambda}" src="../images/tex-74bde878aa116856d62aba260e55c67a.gif" /> 的导数是 <img alt="\lambda x^{\lambda - 1}" src="../images/tex-6c620d50445244971a9718316db37470.gif" /></li>
<li>函数的和的导数，等于函数的导数的和</li>
<li><img alt="\lambda" src="../images/tex-c6a6eb61fd9c6c913da73b3642ca147d.gif" /> 乘以函数，再求导，等于 <img alt="\lambda" src="../images/tex-c6a6eb61fd9c6c913da73b3642ca147d.gif" /> 乘以函数的导数</li>
</ul>
<p>从上述这些规则，可得到公式 D-1。</p>
<p><img alt="公式D-1" src="../images/Appendix/E_D-1.png" /></p>
<p>这个种方法应用于更复杂函数时将变得非常罗嗦，并且有可能出错。好消息是，像刚才我们做的求数学式子的偏导数可以被自动化，通过一个称为符号微分的过程。</p>
<h2 id="_2">符号微分</h2>
<p>图 D-1 展示了符号微分是如何运行在相当简单的函数上的，<img alt="g(x,y) = 5 + xy" src="../images/tex-595a140c599de3ceab7b72d4aaab8a41.gif" />。该函数的计算图如图的左边所示。通过符号微分，我们可得到图的右部分，它代表了 <img alt="\frac{\partial g}{\partial x} = 0 + (0 \times x + y \times 1) = y" src="../images/tex-9e9fa7bbdcb31a3b04a549685db18042.gif" />，相似地也可得到关于<code>y</code>的导数。</p>
<p><img alt="D-1" src="../images/Appendix/D-1.png" /></p>
<p>概算法先获得叶子节点的偏导数。常数 5 返回常数 0，因为常数的导数总是 0。变量<code>x</code>返回常数 1，变量<code>y</code>返回常数 0，因为 <img alt="\frac{\partial y}{\partial x} = 0" src="../images/tex-ea6d21230d9c335a071d341ceb54d780.gif" />（如果我们找关于<code>y</code>的偏导数，那它将反过来）。</p>
<p>现在我们移动到计算图的相乘节点处，代数告诉我们，<code>u</code>和<code>v</code>相乘后的导数为 <img alt="\frac{\partial (u \times v)}{\partial x} = \frac{\partial v}{\partial x} \times u + \frac{\partial u}{\partial x} \times v " src="../images/tex-1cf5205e2548cc4e0ce9e5343ab1a377.gif" />。因此我们可以构造有图中大的部分，代表<code>0 × x + y × 1</code>。</p>
<p>最后我们往上走到计算图的相加节点处，正如 5 条规则里提到的，和的导数等于导数的和。所以我们只需要创建一个相加节点，连接我们已经计算出来的部分。我们可以得到正确的偏导数，即：<img alt="\frac{\partial g}{\partial x} = 0 + (0 \times x + y \times 1) " src="../images/tex-7e03e8e758791a8db7937cbbcc78f2b9.gif" />。</p>
<p>然而，这个过程可简化。对该图应用一些微不足道的剪枝步骤，可以去掉所有不必要的操作，然后我们可以得到一个小得多的只有一个节点的偏导计算图：<img alt="\frac{\partial g}{\partial x} = y" src="../images/tex-1fda7e8979ad0fdf4a2022ee529661d0.gif" />。</p>
<p>在这个例子里，简化操作是相当简单的，但对更复杂的函数来说，符号微分会产生一个巨大的计算图，该图可能很难去简化，以导致次优的性能。更重要的是，符号微分不能处理由任意代码定义的函数，例如，如下已在第 9 章讨论过的函数：</p>
<pre><code class="language-python">def my_func(a, b):
    z = 0
    for i in range(100):
        z = a * np.cos(z + i) + z * np.sin(b - i)
    return z
</code></pre>
<h2 id="_3">数值微分</h2>
<p>从数值上说，最简单的方案是去计算导数的近似值。回忆<code>h(x)</code>在 <img alt="x_0" src="../images/tex-3e0d691f3a530e6c7e079636f20c111b.gif" /> 的导数 <img alt="h^{'}(x_0)" src="../images/tex-6499b5277397390a9878a93fa4205525.gif" />，是该函数在该点处的斜率，或者更准确如公式 D-2 所示。</p>
<p><img alt="E_D-2" src="../images/Appendix/E_D-2.png" /></p>
<p>因此如果我们想要计算 <img alt="f(x,y)" src="../images/tex-3baf1600ae50930a155f58ae172b51bd.gif" /> 关于<code>x</code>，在 <img alt="x=3, y=4" src="../images/tex-99e7bebb7eb398dc777eea8fa1bfe3ba.gif" /> 处的导数，我们可以简单计算 <img alt="f(3+\epsilon, 4) - f(3, 4)" src="../images/tex-5dcd5b36cf658a9fbb13000a4cac6989.gif" /> 的值，将这个结果除以 <img alt="\epsilon" src="../images/tex-92e4da341fe8f4cd46192f21b6ff3aa7.gif" />，且 <img alt="\epsilon" src="../images/tex-92e4da341fe8f4cd46192f21b6ff3aa7.gif" /> 去很小的值。这个过程正是如下的代码所要干的。</p>
<pre><code class="language-python">def f(x, y):
    return x**2*y + y + 2

def derivative(f, x, y, x_eps, y_eps):
    return (f(x + x_eps, y + y_eps) - f(x, y)) / (x_eps + y_eps)

df_dx = derivative(f, 3, 4, 0.00001, 0)
df_dy = derivative(f, 3, 4, 0, 0.00001)
</code></pre>
<p>不幸的是，偏导的结果并不准确（并且可能在求解复杂函数时更糟糕）。上述正确答案分别是 24 和 10 ，但我们得到的是：</p>
<pre><code class="language-python">&gt;&gt;&gt; print(df_dx)
24.000039999805264
&gt;&gt;&gt; print(df_dy)
10.000000000331966
</code></pre>
<p>注意到为了计算两个偏导数， 我们不得不调用<code>f()</code>至少三次（在上述代码里我们调用了四次，但可以优化）。如果存在 1000 个参数，我们将会调用<code>f()</code>至少 1001 次。当处理大的神经网络时，这样的操作很没有效率。</p>
<p>然而，数值微分实现起来如此简单，以至于它是检查其他方法正确性的优秀工具。例如，如果它的结果与您手动计算的导数不同，那么你的导数可能包含错误。</p>
<h2 id="_4">前向自动微分</h2>
<p>前向自动微分既不是数值微分，也不是符号微分，但在某些方面，它是他们的爱情结晶。它依赖对偶数。对偶数是奇怪但迷人的，是 <img alt="a + b\epsilon" src="../images/tex-595b3d916d7b666f7cec8f222f665759.gif" /> 形式的数，这里<code>a</code>和<code>b</code>是实数，<img alt="\epsilon" src="../images/tex-92e4da341fe8f4cd46192f21b6ff3aa7.gif" /> 是无穷小的数，满足 <img alt="\epsilon ^ 2 = 0" src="../images/tex-0fe16f5f8178c40813008f32155da044.gif" />，但 <img alt="\epsilon \ne 0" src="../images/tex-11096ba55e57b0ba1b35efb241f87569.gif" />。你可以认为对偶数 <img alt="42 + 24\epsilon" src="../images/tex-63b17a82b832b929bd916f01c8a4dadd.gif" /> 类似于有着无穷个 0 的 42.0000⋯000024（但当然这是简化后的，仅仅给你对偶数什么的想法）。一个对偶数在内存中表示为一个浮点数对，例如，<img alt="42 + 24\epsilon" src="../images/tex-63b17a82b832b929bd916f01c8a4dadd.gif" /> 表示为<code>(42.0, 24.0)</code>。</p>
<p>对偶数可相加、相乘、等等操作，正如公式 D-3 所示。</p>
<p><img alt="E_D-3" src="../images/Appendix/E_D-3.png" /></p>
<p>最重要的，可证明<code>h(a + bϵ) = h(a) + b × h'(a)ϵ</code>，所以计算一次<code>h(a + ϵ)</code>就得到了两个值<code>h(a)</code>和<code>h'(a)</code>。图 D-2 展示了前向自动微分如何计算 <img alt="f(x,y)=x^2y + y + 2" src="../images/tex-bf7d4f41a093293adbb04e43c7d12839.gif" /> 关于<code>x</code>，在 <img alt="x=3, y=4" src="../images/tex-99e7bebb7eb398dc777eea8fa1bfe3ba.gif" /> 处的导数。我们所要做的一切只是计算 <img alt="f(3+\epsilon, 4)" src="../images/tex-da5577f9751e71377558278256ff1115.gif" />；它将输出一个对偶数，其第一部分等于 <img alt="f(3, 4)" src="../images/tex-744a84046c00c267c037276ee9483cff.gif" />，第二部分等于 <img alt="f^{'}(3, 4) = \frac{\partial f}{\partial x} (3,4)" src="../images/tex-399b8bab86aa930cdbf5c93b2e3fa818.gif" />。</p>
<p><img alt="D-2" src="../images/Appendix/D-2.png" /></p>
<p>为了计算 <img alt="\frac{\partial f}{\partial y} (3,4)" src="../images/tex-3b5f49ee9fe10430f81eeef7000f1b30.gif" /> 我们不得不再遍历一遍计算图，但这次前馈的值为 <img alt="x=3, y = 4 + \epsilon" src="../images/tex-a6ef39467ae1ecfdf09a7e93357c3154.gif" />。</p>
<p>所以前向自动微分比数值微分准确得多，但它遭受同样的缺陷：如果有 1000 个参数，那为了计算所有的偏导数，得历经计算图 1000 次。这正是反向自动微分耀眼的地方：计算所有的偏导数，它只需要遍历计算图 2 次。</p>
<h2 id="_5">反向自动微分</h2>
<p>反向自动微分是 TensorFlow 采取的方案。它首先前馈遍历计算图（即，从输入到输出），计算出每个节点的值。然后进行第二次遍历，这次是反向遍历（即，从输出到输入），计算出所有的偏导数。图 D-3 展示了第二次遍历的过程。在第一次遍历过程中，所有节点值已被计算，输入是 <img alt="x=3, y=4" src="../images/tex-99e7bebb7eb398dc777eea8fa1bfe3ba.gif" />。你可以在每个节点底部右方看到这些值（例如，<img alt="x \times x = 9" src="../images/tex-ddfd45b07cca3862ad001dc6551d826a.gif" />）。节点已被标号，从 <img alt="n_1" src="../images/tex-6c773b2b7798e5713845e475d0c4b4c7.gif" /> 到 <img alt="n_7" src="../images/tex-97d045dcd64af5ae4cc4add328629288.gif" />。输出节点是 <img alt="n_7: f(3, 4) = n_7 = 42" src="../images/tex-17241d7ea090e8a7be55cacfcd5b2768.gif" />。</p>
<p><img alt="D-3" src="../images/Appendix/D-3.png" /></p>
<p>这个计算关于每个连续节点的偏导数的思想逐渐地从上到下遍历图，直到到达变量节点。为实现这个，反向自动微分强烈依赖于链式法则，如公式 D-4 所示。</p>
<p><img alt="E_D-4" src="../images/Appendix/E_D-4.png" /></p>
<p>由于 <img alt="n_7" src="../images/tex-97d045dcd64af5ae4cc4add328629288.gif" /> 是输出节点，即 <img alt="f= n_7" src="../images/tex-9233369b2eac1c4808ae768a0534fa78.gif" />，所以 <img alt="\frac{\partial f}{\partial n_7} = 1" src="../images/tex-c052878d41402368d536c53f4937b012.gif" />。</p>
<p>接着到了图的 <img alt="n_5" src="../images/tex-53eba210fc14ef60860265ec70fb718d.gif" /> 节点：当 <img alt="n_5" src="../images/tex-53eba210fc14ef60860265ec70fb718d.gif" /> 变化时，<img alt="f" src="../images/tex-8fa14cdd754f91cc6554c9e71929cce7.gif" /> 会变化多少？答案是 <img alt="\frac{\partial f}{\partial n_5} = \frac{\partial f}{\partial n_7} \times \frac{\partial n_7}{\partial n_5}" src="../images/tex-c4664533339cdf3ddbe912caf82c5bdc.gif" />。我们已经知道 <img alt="\frac{\partial f}{\partial n_7} = 1" src="../images/tex-c052878d41402368d536c53f4937b012.gif" />，因此我们只需要知道 <img alt="\frac{\partial n_7}{\partial n_5}" src="../images/tex-3d189a2e226493acc6538bcd3e9cb366.gif" /> 就行。因为 <img alt="n_7" src="../images/tex-97d045dcd64af5ae4cc4add328629288.gif" /> 是 <img alt="n_5 + n_6" src="../images/tex-bf018abe4e43c0b3132cba23cb971907.gif" /> 的和，因此可得到 <img alt="\frac{\partial n_7}{\partial n_5} = 1" src="../images/tex-68f34602f87a1f0669551323e59a17ea.gif" />，因此 <img alt="\frac{\partial f}{\partial n_5}=1 \times 1 = 1" src="../images/tex-d0a7f1641b3fe72530efcea74fd7a4d2.gif" />。</p>
<p>现在前进到 <img alt="n_4" src="../images/tex-43c5783d36b015e36edeecd60da73206.gif" />：当 <img alt="n_4" src="../images/tex-43c5783d36b015e36edeecd60da73206.gif" /> 变化时，<img alt="f" src="../images/tex-8fa14cdd754f91cc6554c9e71929cce7.gif" /> 会变化多少？答案是 <img alt="\frac{\partial f}{\partial n_4} = \frac{\partial f}{\partial n_5} \times \frac{\partial n_5}{\partial n_4}" src="../images/tex-414889b175f816852566907db5edd6a5.gif" />。由于 <img alt="n_5 = n_4 \times n_2" src="../images/tex-c982adc41e9ee58af9aed4995717fa82.gif" />，我们可得到 <img alt="\frac{\partial n_5}{\partial n_4} = n_2" src="../images/tex-421556b6c8203ded772656e90a1a570c.gif" />，所以 <img alt="\frac{\partial f}{\partial n_4}= 1 \times n_2 = 4" src="../images/tex-5da5d4cf0bebe9ea96d3fbb2c2fd93ca.gif" />。</p>
<p>这个遍历过程一直持续，此时我们达到图的底部。这时我们已经得到了所有偏导数在点 <img alt="x=3, y=4" src="../images/tex-99e7bebb7eb398dc777eea8fa1bfe3ba.gif" /> 处的值。在这个例子里，我们得到 <img alt="\frac{\partial f}{\partial x} = 24, \frac{\partial f}{\partial y} = 10" src="../images/tex-e39fd6874bfece3703cdd1eb53e170b0.gif" />。听起来很美妙！</p>
<p>反向自动微分是非常强大且准确的技术，尤其是当有很多输入参数和极少输出时，因为它只要求一次前馈传递加上一次反向传递，就可计算所有输出关于所有输入的偏导数。最重要的是，它可以处理任意代码定义的函数。它也可以处理那些不完全可微的函数，只要  你要求他计算的偏导数在该点处是可微的。</p>
<p>如果你在 TensorFlow 中实现了新算子，你想使它与现有的自动微分相兼容，那你需要提供函数，该函数用于构建一个子图，来计算关于新算子输入的偏导数。例如，假设你实现了一个计算其输入的平方的函数，平方算子 <img alt="f(x)= x ^2" src="../images/tex-d26940d88870bfe622e50be50381fdb9.gif" />，在这个例子中你需要提供相应的导函数 <img alt="f^{'}(x)= 2x " src="../images/tex-8f515dd3c20d16c5ed6223da611b9a2f.gif" />。注意这个导函数不计算一个数值结果，而是用于构建子图，该子图后续将计算偏导结果。这是非常有用的，因为这意味着你可以计算梯度的梯度（为了计算二阶导数，或者甚至更高阶的导数）。</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="../C.SVM_%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98/" class="btn btn-neutral" title="C.SVM_对偶问题"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../C.SVM_%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
